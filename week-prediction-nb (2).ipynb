{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ec991e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T09:40:02.126368Z",
     "iopub.status.busy": "2024-05-18T09:40:02.125982Z",
     "iopub.status.idle": "2024-05-18T09:40:06.668473Z",
     "shell.execute_reply": "2024-05-18T09:40:06.666972Z"
    },
    "papermill": {
     "duration": 4.554725,
     "end_time": "2024-05-18T09:40:06.671675",
     "exception": false,
     "start_time": "2024-05-18T09:40:02.116950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import lightgbm as lgb  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "import pandas as pd  # type: ignore\n",
    "import polars as pl  # type: ignore\n",
    "import warnings\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from catboost import CatBoostClassifier, Pool  # type: ignore\n",
    "from glob import glob\n",
    "from IPython.display import display  # type: ignore\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin  # type: ignore\n",
    "from sklearn.metrics import roc_auc_score  # type: ignore\n",
    "from sklearn.model_selection import StratifiedGroupKFold  # type: ignore\n",
    "from typing import Any\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ROOT = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "TRAIN_DIR = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR = ROOT / \"parquet_files\" / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd7d1fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T09:40:06.685801Z",
     "iopub.status.busy": "2024-05-18T09:40:06.685181Z",
     "iopub.status.idle": "2024-05-18T09:40:06.728842Z",
     "shell.execute_reply": "2024-05-18T09:40:06.727365Z"
    },
    "papermill": {
     "duration": 0.054385,
     "end_time": "2024-05-18T09:40:06.732151",
     "exception": false,
     "start_time": "2024-05-18T09:40:06.677766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Utility:\n",
    "    @staticmethod\n",
    "    def get_feat_defs(ending_with: str) -> None:\n",
    "        \"\"\"\n",
    "        Retrieves feature definitions from a CSV file based on the specified ending.\n",
    "\n",
    "        Args:\n",
    "        - ending_with (str): Ending to filter feature definitions.\n",
    "\n",
    "        Returns:\n",
    "        - pl.DataFrame: Filtered feature definitions.\n",
    "        \"\"\"\n",
    "        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n",
    "\n",
    "        filtered_feats: pl.DataFrame = feat_defs.filter(\n",
    "            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n",
    "        )\n",
    "\n",
    "        with pl.Config(fmt_str_lengths=200, tbl_rows=-1):\n",
    "            print(filtered_feats)\n",
    "\n",
    "        filtered_feats = None\n",
    "        feat_defs = None\n",
    "\n",
    "    @staticmethod\n",
    "    def find_index(lst: list[Any], item: Any) -> int | None:\n",
    "        \"\"\"\n",
    "        Finds the index of an item in a list.\n",
    "\n",
    "        Args:\n",
    "        - lst (list): List to search.\n",
    "        - item (Any): Item to find in the list.\n",
    "\n",
    "        Returns:\n",
    "        - int | None: Index of the item if found, otherwise None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return lst.index(item)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def dtype_to_str(dtype: pl.DataType) -> str:\n",
    "        \"\"\"\n",
    "        Converts Polars data type to string representation.\n",
    "\n",
    "        Args:\n",
    "        - dtype (pl.DataType): Polars data type.\n",
    "\n",
    "        Returns:\n",
    "        - str: String representation of the data type.\n",
    "        \"\"\"\n",
    "        dtype_map = {\n",
    "            pl.Decimal: \"Decimal\",\n",
    "            pl.Float32: \"Float32\",\n",
    "            pl.Float64: \"Float64\",\n",
    "            pl.UInt8: \"UInt8\",\n",
    "            pl.UInt16: \"UInt16\",\n",
    "            pl.UInt32: \"UInt32\",\n",
    "            pl.UInt64: \"UInt64\",\n",
    "            pl.Int8: \"Int8\",\n",
    "            pl.Int16: \"Int16\",\n",
    "            pl.Int32: \"Int32\",\n",
    "            pl.Int64: \"Int64\",\n",
    "            pl.Date: \"Date\",\n",
    "            pl.Datetime: \"Datetime\",\n",
    "            pl.Duration: \"Duration\",\n",
    "            pl.Time: \"Time\",\n",
    "            pl.Array: \"Array\",\n",
    "            pl.List: \"List\",\n",
    "            pl.Struct: \"Struct\",\n",
    "            pl.String: \"String\",\n",
    "            pl.Categorical: \"Categorical\",\n",
    "            pl.Enum: \"Enum\",\n",
    "            pl.Utf8: \"Utf8\",\n",
    "            pl.Binary: \"Binary\",\n",
    "            pl.Boolean: \"Boolean\",\n",
    "            pl.Null: \"Null\",\n",
    "            pl.Object: \"Object\",\n",
    "            pl.Unknown: \"Unknown\",\n",
    "        }\n",
    "\n",
    "        return dtype_map.get(dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_feat_occur(regex_path: str, ending_with: str) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Finds occurrences of features ending with a specific string in Parquet files.\n",
    "\n",
    "        Args:\n",
    "        - regex_path (str): Regular expression to match Parquet file paths.\n",
    "        - ending_with (str): Ending to filter feature names.\n",
    "\n",
    "        Returns:\n",
    "        - pl.DataFrame: DataFrame containing feature definitions, data types, and file locations.\n",
    "        \"\"\"\n",
    "        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\").filter(\n",
    "            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n",
    "        )\n",
    "        feat_defs.sort(by=[\"Variable\"])\n",
    "\n",
    "        feats: list[pl.String] = feat_defs[\"Variable\"].to_list()\n",
    "        feats.sort()\n",
    "\n",
    "        occurrences: list[list] = [[set(), set()] for _ in range(feat_defs.height)]\n",
    "\n",
    "        for path in glob(str(regex_path)):\n",
    "            df_schema: dict = pl.read_parquet_schema(path)\n",
    "\n",
    "            for feat, dtype in df_schema.items():\n",
    "                index: int = Utility.find_index(feats, feat)\n",
    "                if index != None:\n",
    "                    occurrences[index][0].add(Utility.dtype_to_str(dtype))\n",
    "                    occurrences[index][1].add(Path(path).stem)\n",
    "\n",
    "        data_types: list[str] = [None] * feat_defs.height\n",
    "        file_locs: list[str] = [None] * feat_defs.height\n",
    "\n",
    "        for i, feat in enumerate(feats):\n",
    "            data_types[i] = list(occurrences[i][0])\n",
    "            file_locs[i] = list(occurrences[i][1])\n",
    "\n",
    "        feat_defs = feat_defs.with_columns(pl.Series(data_types).alias(\"Data_Type(s)\"))\n",
    "        feat_defs = feat_defs.with_columns(pl.Series(file_locs).alias(\"File_Loc(s)\"))\n",
    "\n",
    "        return feat_defs\n",
    "\n",
    "    def reduce_memory_usage(df: pl.DataFrame, name) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Reduces memory usage of a DataFrame by converting column types.\n",
    "\n",
    "        Args:\n",
    "        - df (pl.DataFrame): DataFrame to optimize.\n",
    "        - name (str): Name of the DataFrame.\n",
    "\n",
    "        Returns:\n",
    "        - pl.DataFrame: Optimized DataFrame.\n",
    "        \"\"\"\n",
    "        print(\n",
    "            f\"Memory usage of dataframe \\\"{name}\\\" is {round(df.estimated_size('mb'), 4)} MB.\"\n",
    "        )\n",
    "\n",
    "        int_types = [\n",
    "            pl.Int8,\n",
    "            pl.Int16,\n",
    "            pl.Int32,\n",
    "            pl.Int64,\n",
    "            pl.UInt8,\n",
    "            pl.UInt16,\n",
    "            pl.UInt32,\n",
    "            pl.UInt64,\n",
    "        ]\n",
    "        float_types = [pl.Float32, pl.Float64]\n",
    "\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "            if col_type in int_types + float_types:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "\n",
    "                if c_min is not None and c_max is not None:\n",
    "                    if col_type in int_types:\n",
    "                        if c_min >= 0:\n",
    "                            if (\n",
    "                                c_min >= np.iinfo(np.uint8).min\n",
    "                                and c_max <= np.iinfo(np.uint8).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.UInt8))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.uint16).min\n",
    "                                and c_max <= np.iinfo(np.uint16).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.UInt16))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.uint32).min\n",
    "                                and c_max <= np.iinfo(np.uint32).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.UInt32))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.uint64).min\n",
    "                                and c_max <= np.iinfo(np.uint64).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.UInt64))\n",
    "                        else:\n",
    "                            if (\n",
    "                                c_min >= np.iinfo(np.int8).min\n",
    "                                and c_max <= np.iinfo(np.int8).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.Int8))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.int16).min\n",
    "                                and c_max <= np.iinfo(np.int16).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.Int16))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.int32).min\n",
    "                                and c_max <= np.iinfo(np.int32).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.Int32))\n",
    "                            elif (\n",
    "                                c_min >= np.iinfo(np.int64).min\n",
    "                                and c_max <= np.iinfo(np.int64).max\n",
    "                            ):\n",
    "                                df = df.with_columns(df[col].cast(pl.Int64))\n",
    "                    elif col_type in float_types:\n",
    "                        if (\n",
    "                            c_min > np.finfo(np.float32).min\n",
    "                            and c_max < np.finfo(np.float32).max\n",
    "                        ):\n",
    "                            df = df.with_columns(df[col].cast(pl.Float32))\n",
    "\n",
    "        print(\n",
    "            f\"Memory usage of dataframe \\\"{name}\\\" became {round(df.estimated_size('mb'), 4)} MB.\"\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def to_pandas(df: pl.DataFrame, cat_cols: list[str] = None) -> (pd.DataFrame, list[str]):  # type: ignore\n",
    "        \"\"\"\n",
    "        Converts a Polars DataFrame to a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "        - df (pl.DataFrame): Polars DataFrame to convert.\n",
    "        - cat_cols (list[str]): List of categorical columns. Default is None.\n",
    "\n",
    "        Returns:\n",
    "        - (pd.DataFrame, list[str]): Tuple containing the converted Pandas DataFrame and categorical columns.\n",
    "        \"\"\"\n",
    "        df: pd.DataFrame = df.to_pandas()\n",
    "\n",
    "        if cat_cols is None:\n",
    "            cat_cols = list(df.select_dtypes(\"object\").columns)\n",
    "\n",
    "        df[cat_cols] = df[cat_cols].astype(\"str\")\n",
    "\n",
    "        return df, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd3faa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T09:40:06.745857Z",
     "iopub.status.busy": "2024-05-18T09:40:06.745410Z",
     "iopub.status.idle": "2024-05-18T09:40:06.751119Z",
     "shell.execute_reply": "2024-05-18T09:40:06.749733Z"
    },
    "papermill": {
     "duration": 0.015208,
     "end_time": "2024-05-18T09:40:06.753370",
     "exception": false,
     "start_time": "2024-05-18T09:40:06.738162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"P\")\n",
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"M\")\n",
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"A\")\n",
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"D\")\n",
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"T\")\n",
    "# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"L\")\n",
    "# feat_defs:pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n",
    "# with pl.Config(fmt_str_lengths=1000, tbl_rows=-1, tbl_width_chars=180):\n",
    "#     print(feat_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367ad8bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T09:40:06.766794Z",
     "iopub.status.busy": "2024-05-18T09:40:06.766368Z",
     "iopub.status.idle": "2024-05-18T09:40:41.724925Z",
     "shell.execute_reply": "2024-05-18T09:40:41.723652Z"
    },
    "papermill": {
     "duration": 34.969939,
     "end_time": "2024-05-18T09:40:41.729147",
     "exception": false,
     "start_time": "2024-05-18T09:40:06.759208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train=pl.read_parquet('/kaggle/input/credit-risk-prediction-train-data/train_final.parquet')\n",
    "df_train=df_train[[col for col in df_train.columns if 'crb2' not in col.split('_') and 'crb' not in col.split('_') and 'statcb' not in col.split('_')]]\n",
    "df_train, cat_cols = Utility.to_pandas(df_train)\n",
    "pd.Series(cat_cols).to_csv('cat_cols.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb56fed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T09:40:41.744158Z",
     "iopub.status.busy": "2024-05-18T09:40:41.743731Z",
     "iopub.status.idle": "2024-05-18T09:40:41.753743Z",
     "shell.execute_reply": "2024-05-18T09:40:41.752852Z"
    },
    "papermill": {
     "duration": 0.020311,
     "end_time": "2024-05-18T09:40:41.756159",
     "exception": false,
     "start_time": "2024-05-18T09:40:41.735848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VotingModel(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    A voting ensemble model that combines predictions from multiple estimators.\n",
    "\n",
    "    Parameters:\n",
    "    - estimators (list): List of base estimators.\n",
    "\n",
    "    Attributes:\n",
    "    - estimators (list): List of base estimators.\n",
    "\n",
    "    Methods:\n",
    "    - fit(X, y=None): Fit the model to the training data.\n",
    "    - predict(X): Predict class labels for samples.\n",
    "    - predict_proba(X): Predict class probabilities for samples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimators: list[BaseEstimator]):\n",
    "        \"\"\"\n",
    "        Initialize the VotingModel with a list of base estimators.\n",
    "\n",
    "        Args:\n",
    "        - estimators (list): List of base estimators.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the model to the training data.\n",
    "\n",
    "        Args:\n",
    "        - X: Input features.\n",
    "        - y: Target labels (ignored).\n",
    "\n",
    "        Returns:\n",
    "        - self: Returns the instance itself.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples.\n",
    "\n",
    "        Args:\n",
    "        - X: Input features.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Predicted class labels.\n",
    "        \"\"\"\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for samples.\n",
    "\n",
    "        Args:\n",
    "        - X: Input features.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Predicted class probabilities.\n",
    "        \"\"\"\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a6fafc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T09:40:41.770165Z",
     "iopub.status.busy": "2024-05-18T09:40:41.769747Z",
     "iopub.status.idle": "2024-05-18T09:40:41.774741Z",
     "shell.execute_reply": "2024-05-18T09:40:41.773600Z"
    },
    "papermill": {
     "duration": 0.015337,
     "end_time": "2024-05-18T09:40:41.777348",
     "exception": false,
     "start_time": "2024-05-18T09:40:41.762011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device='gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf00cc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T09:40:41.790918Z",
     "iopub.status.busy": "2024-05-18T09:40:41.790503Z",
     "iopub.status.idle": "2024-05-18T09:40:41.795239Z",
     "shell.execute_reply": "2024-05-18T09:40:41.793898Z"
    },
    "papermill": {
     "duration": 0.014996,
     "end_time": "2024-05-18T09:40:41.798188",
     "exception": false,
     "start_time": "2024-05-18T09:40:41.783192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e5ce06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T09:40:41.812568Z",
     "iopub.status.busy": "2024-05-18T09:40:41.811643Z",
     "iopub.status.idle": "2024-05-18T09:56:08.425373Z",
     "shell.execute_reply": "2024-05-18T09:56:08.423156Z"
    },
    "papermill": {
     "duration": 926.623996,
     "end_time": "2024-05-18T09:56:08.428375",
     "exception": false,
     "start_time": "2024-05-18T09:40:41.804379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 09:40:43.944659: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-18 09:40:43.944829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-18 09:40:44.089809: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m47709/47709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 3ms/step - loss: 18.3431\n",
      "Epoch 2/7\n",
      "\u001b[1m47709/47709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 3ms/step - loss: 15.6529\n",
      "Epoch 3/7\n",
      "\u001b[1m47709/47709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 3ms/step - loss: 15.1650\n",
      "Epoch 4/7\n",
      "\u001b[1m47709/47709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 3ms/step - loss: 14.9854\n",
      "Epoch 5/7\n",
      "\u001b[1m47709/47709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 3ms/step - loss: 14.7659\n",
      "Epoch 6/7\n",
      "\u001b[1m47709/47709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 3ms/step - loss: 14.6027\n",
      "Epoch 7/7\n",
      "\u001b[1m47709/47709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 3ms/step - loss: 14.4605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7c49d8673040>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,LayerNormalization, Dropout\n",
    "import tensorflow.keras.backend as K\n",
    "# For custom RMSE metric\n",
    "feat=[col for col in df_train.columns if col not in cat_cols+['target','case_id', 'month', 'week_num', 'day']]\n",
    "#df_train[cat_cols]=df_train[cat_cols].astype('category')\n",
    "random_seed = 42\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "pd.Series(feat).to_csv('feat.csv', index=False)\n",
    "weeks = df_train[\"week_num\"]\n",
    "# Custom RMSE metric function (optional)\n",
    "@keras.saving.register_keras_serializable()\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Create a deep neural network model\n",
    "model = Sequential()\n",
    "model.add(LayerNormalization(input_dim=df_train[feat].shape[1]))  # First hidden layer with 128 units and ReLU activation\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu')) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu')) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))  # Output layer with 1 unit for continuous prediction\n",
    "\n",
    "# Compile the model with custom RMSE loss and Adam optimizer\n",
    "model.compile(loss=rmse, optimizer='adam')\n",
    "\n",
    "# Train the model (adjust epochs as needed)\n",
    "model.fit(df_train[feat].fillna(0),df_train['week_num'], epochs=7, \n",
    "          #validation_data=(df_train[feat].fillna(0),df_train[split:]['week_num'])\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34682649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T07:22:01.241532Z",
     "iopub.status.busy": "2024-05-18T07:22:01.241076Z",
     "iopub.status.idle": "2024-05-18T07:22:16.413499Z",
     "shell.execute_reply": "2024-05-18T07:22:16.411508Z",
     "shell.execute_reply.started": "2024-05-18T07:22:01.241455Z"
    },
    "papermill": {
     "duration": 1.776747,
     "end_time": "2024-05-18T09:56:11.891508",
     "exception": false,
     "start_time": "2024-05-18T09:56:10.114761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "feat=[col for col in df_train.columns if col not in cat_cols+['target','case_id', 'month', 'week_num', 'day']]\n",
    "#df_train[cat_cols]=df_train[cat_cols].astype('category')\n",
    "pd.Series(feat).to_csv('feat.csv', index=False)\n",
    "weeks = df_train[\"week_num\"]\n",
    "\n",
    "fitted_models_lgb = []\n",
    "model = Lasso()\n",
    "model.fit(\n",
    "        df_train[feat].fillna(0),\n",
    "        df_train['week_num'],\n",
    "        #eval_set=[(df_train[feat][split:], df_train['week_num'][split:])],\n",
    "        #callbacks=[lgb.log_evaluation(1), lgb.early_stopping(100)],\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecd5b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T05:55:19.106765Z",
     "iopub.status.busy": "2024-05-18T05:55:19.106374Z",
     "iopub.status.idle": "2024-05-18T05:57:32.068523Z",
     "shell.execute_reply": "2024-05-18T05:57:32.067536Z",
     "shell.execute_reply.started": "2024-05-18T05:55:19.106735Z"
    },
    "papermill": {
     "duration": 1.760487,
     "end_time": "2024-05-18T09:56:15.422343",
     "exception": false,
     "start_time": "2024-05-18T09:56:13.661856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "feat=[col for col in df_train.columns if col not in ['target','case_id', 'month', 'week_num', 'day']]\n",
    "df_train[cat_cols]=df_train[cat_cols].astype('category')\n",
    "pd.Series(feat).to_csv('feat.csv', index=False)\n",
    "weeks = df_train[\"week_num\"]\n",
    "params1 = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"device\": device,\n",
    "    \"extra_trees\": True,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"l1_regularization\": 0.1,\n",
    "    \"l2_regularization\": 10,\n",
    "    \"max_depth\": 20,\n",
    "    #\"metric\": \"auc\",\n",
    "    \"n_estimators\": 700,\n",
    "    \"num_leaves\": 64,\n",
    "    #\"objective\": \"binary\",\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "fitted_models_lgb = []\n",
    "model = lgb.LGBMRegressor(**params1)\n",
    "model.fit(\n",
    "        df_train[feat],\n",
    "        df_train['week_num'],\n",
    "        #eval_set=[(df_train[feat][split:], df_train['week_num'][split:])],\n",
    "        #callbacks=[lgb.log_evaluation(1), lgb.early_stopping(100)],\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94ba27c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T09:56:18.909605Z",
     "iopub.status.busy": "2024-05-18T09:56:18.909041Z",
     "iopub.status.idle": "2024-05-18T09:56:18.916157Z",
     "shell.execute_reply": "2024-05-18T09:56:18.914829Z"
    },
    "papermill": {
     "duration": 1.739202,
     "end_time": "2024-05-18T09:56:18.918459",
     "exception": false,
     "start_time": "2024-05-18T09:56:17.179257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pd.Series(model.feature_importances_, feat).sort_values(ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a66ae8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T09:56:22.453790Z",
     "iopub.status.busy": "2024-05-18T09:56:22.453392Z",
     "iopub.status.idle": "2024-05-18T09:56:22.538390Z",
     "shell.execute_reply": "2024-05-18T09:56:22.537050Z"
    },
    "papermill": {
     "duration": 1.865335,
     "end_time": "2024-05-18T09:56:22.541527",
     "exception": false,
     "start_time": "2024-05-18T09:56:20.676192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08944ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T08:16:54.492447Z",
     "iopub.status.busy": "2024-05-18T08:16:54.492015Z",
     "iopub.status.idle": "2024-05-18T08:16:56.348947Z",
     "shell.execute_reply": "2024-05-18T08:16:56.347234Z",
     "shell.execute_reply.started": "2024-05-18T08:16:54.492407Z"
    },
    "papermill": {
     "duration": 1.680243,
     "end_time": "2024-05-18T09:56:25.993604",
     "exception": false,
     "start_time": "2024-05-18T09:56:24.313361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    },
    {
     "datasetId": 5018384,
     "sourceId": 8427693,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 991.639599,
   "end_time": "2024-05-18T09:56:30.703234",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-18T09:39:59.063635",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
